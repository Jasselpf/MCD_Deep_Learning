---
title: "DL_20200206"
author: "Soledad Perez"
date: "February 6, 2020"
output: html_document
---



## Deep Learning - clase febrero 2, 2020


### Optimizadores

#### Gradient Descent (GD)

Descenso en gradiente es un algoritmo de optimización usado para minimizar alguna función al moverse itarativamente 
en la direción de mayor descenso (steppest descent) definido por el negativo del gradiente. 

$$\Omega = \Omega - \eta \nabla J(\Omega)$$
$$\omega_i = \omega_i - \eta \frac{\partial E}{\partial \omega_i}$$

$\eta:$ taza de aprendizaje

#### Stochastic Gradient Descent



### Preguntas posibles para el exámen

1. ¿Qué es deep learning? \

Deep Learning es una técnica de aprendizaje automático (machine leanring) basada en aprender características
y tareas directamente de los datos. Se caracteriza por estar compuesto por redes neuronales para el 
procesaminento de la información.


2. ¿Qué lo hace deep learning? \

La principal distinción del deep learning se establece por su estructura y procesamiento de la información el cual imita las 
redes neuronales del cerebro humano. El término deep hace referencia a las capas ocultas de estas redes, que generalmente 
son del orden de decenas a centenas.


3. Relación entre regresión y clasificación. ¿Por qué son modelos equivalentes? \


4. Conjuntos de datos Test y Prueba \



5. Diferencia entre parámetro o hiperparámetro 


6. Perceptrón. Idea, ¿qué modela?, ¿cómo calculo el número de parámetros? y ¿qué debo considerar?



7. Backprop, si tengo una red de este tipo, ¿qué parametros debo de considerar?




8. Vanishing gradient / Exploding gradient problem

Si la función de activacion tiene una derivada que pueda tomar valores muy grandes, uno de los riesgos que se presentan es 
el llamado exploding gradient problem


9. Funciones de activación, ¿cuándo es recomendable usar una u otra?



10. Funciones de pérdida, ¿cuándo conviene usar una u otra?



11. Optimizadores











